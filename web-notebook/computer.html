<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      Low-Fidelity Prototype & Usability Test Report — Gaze-Aware Immersive
      Video Streaming
    </title>
    <style>
      body {
        font-family: "Helvetica Neue", Arial, sans-serif;
        line-height: 1.6;
        background-color: #f8f9fa;
        color: #333;
        margin: 0;
        padding: 0;
      }
      .container {
        max-width: 900px;
        margin: 40px auto;
        background: #fff;
        padding: 40px;
        box-shadow: 0 0 8px rgba(0, 0, 0, 0.1);
        border-radius: 8px;
      }
      .imagebox {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
      }
      h1,
      h2,
      h3,
      h4 {
        color: #222;
        font-weight: 600;
        margin-top: 1.5em;
      }
      h1 {
        text-align: center;
        margin-bottom: 0.5em;
      }
      h2 {
        border-bottom: 2px solid #e5e5e5;
        padding-bottom: 4px;
        margin-bottom: 0.6em;
      }
      p,
      ul,
      ol {
        margin-bottom: 1em;
      }
      ul ul {
        margin-bottom: 0em;
      }
      ul {
        padding-left: 1.2em;
      }
      table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 1em;
      }
      th,
      td {
        border: 1px solid #ddd;
        padding: 8px 10px;
        vertical-align: top;
      }
      th {
        background-color: #f2f2f2;
        text-align: left;
      }
      footer {
        text-align: center;
        font-size: 0.9em;
        color: #777;
        margin-top: 3em;
      }
      pre {
        background: #f4f6f8;
        padding: 12px;
        border-radius: 6px;
        overflow: auto;
      }
      nav ul li a:hover {
        background: #f3f4f6;
        color: #0078d4;
      }
      nav ul li a.active {
        border-bottom: 3px solid #0078d4;
        color: #0078d4;
      }
      .overview-figure {
        margin: 2em auto 2.5em;
        text-align: center;
      }
      .overview-figure img {
        max-width: 100%;
        height: auto;
        border-radius: 12px;
        box-shadow: 0 10px 28px rgba(15, 23, 42, 0.16);
        border: 1px solid #e5e7eb;
      }
      .overview-figure figcaption {
        margin-top: 0.75em;
        color: #666;
        font-size: 0.95em;
      }
      .video-wrapper {
        position: relative;
        width: 100%;
        padding-top: 56.25%;
        border-radius: 12px;
        overflow: hidden;
        background: #000;
        box-shadow: 0 10px 28px rgba(15, 23, 42, 0.16);
        border: 1px solid #e5e7eb;
        margin: 1.8em 0;
      }
      .video-wrapper video {
        position: absolute;
        inset: 0;
        width: 100%;
        height: 100%;
        object-fit: cover;
      }
      .section-header {
        text-align: center;
        margin-bottom: 2.5em;
      }
      .section-header p {
        color: #555;
        margin-top: 0.5em;
        font-size: 1.05em;
      }
      .section-lead {
        margin-top: 0.4em;
        color: #555;
        font-size: 1.05em;
      }
      .pdf-grid {
        display: grid;
        gap: 32px;
        grid-template-columns: 1fr;
        margin-top: 1.5em;
      }
      .pdf-card {
        background: #fafbfd;
        border: 1px solid #e5e7eb;
        border-radius: 10px;
        padding: 24px;
        box-shadow: 0 4px 12px rgba(15, 23, 42, 0.06);
      }
      .pdf-card h3 {
        margin-top: 0;
        margin-bottom: 0.4em;
        font-size: 1.05em;
      }
      .pdf-card p {
        margin-bottom: 0.8em;
        color: #555;
      }
      .pdf-card a {
        color: #0066cc;
        text-decoration: none;
        font-weight: 600;
      }
      .pdf-card a:hover {
        text-decoration: underline;
      }
      .pdf-embed {
        width: 100%;
        height: 520px;
        border: none;
        border-radius: 8px;
        background: #ffffff;
        box-shadow: inset 0 0 0 1px #e5e7eb;
      }
    </style>
  </head>
  <body>
    <nav
      style="
        background: #ffffff;
        border-bottom: 1px solid #e2e2e2;
        box-shadow: 0 1px 4px rgba(0, 0, 0, 0.05);
        position: sticky;
        top: 0;
        z-index: 100;
      "
    >
      <ul
        style="
          display: flex;
          justify-content: center;
          list-style: none;
          margin: 0;
          padding: 0;
        "
      >
        <li style="margin: 0">
          <a
            href="index.html"
            style="
              display: block;
              padding: 14px 18px;
              text-decoration: none;
              color: #333;
              font-weight: 500;
            "
            >Observation and Proposal</a
          >
        </li>
        <li style="margin: 0">
          <a
            href="low.html"
            style="
              display: block;
              padding: 14px 18px;
              text-decoration: none;
              color: #333;
              font-weight: 500;
            "
            >Low-Fidelity Prototype</a
          >
        </li>
        <li style="margin: 0">
          <a
            href="computer.html"
            style="
              display: block;
              padding: 14px 18px;
              text-decoration: none;
              color: #333;
              font-weight: 500;
              border-bottom: 3px solid #0078d4;
            "
            >Computer Prototype</a
          >
        </li>
        <li style="margin: 0">
          <a
            href="feedback.html"
            style="
              display: block;
              padding: 14px 18px;
              text-decoration: none;
              color: #333;
              font-weight: 500;
            "
            >Formative Feedback</a
          >
        </li>
        <li style="margin: 0">
          <a
            href="alpha.html"
            style="
              display: block;
              padding: 14px 18px;
              text-decoration: none;
              color: #333;
              font-weight: 500;
            "
            >Alpha System</a
          >
        </li>
        <li style="margin: 0">
          <a
            href="beta.html"
            style="
              display: block;
              padding: 14px 18px;
              text-decoration: none;
              color: #333;
              font-weight: 500;
            "
            >Beta System</a
          >
        </li>
      </ul>
    </nav>
    <div class="container">
      <div class="section-header">
        <h1>Computer Prototype</h1>
        <p>
          A digital walkthrough of the gaze-aware streaming experience with
          supporting documentation for evaluators and participants.
        </p>
      </div>

      <section>
        <h2>1. Prototype Overview</h2>
        <h3>1.1 System Description</h3>
        <p>
          The current build, the “Gaze Interaction Prototype Suite,” packages
          three gaze-inspired interaction concepts into a single HTML experience
          that stakeholders can launch without additional tooling.
        </p>
        <p>
          <a
            href="computer%20prototype.html"
            target="_blank"
            rel="noopener"
            style="color: #0066cc; font-weight: 600; text-decoration: none"
          >
            Launch the computer prototype in a new tab
          </a>
        </p>
        <ul>
          <li>
            <strong>Gaze-Adaptive Player.</strong> A focus spotlight trails the
            viewer’s point of regard, sharpening the focal region while
            softening the periphery.
          </li>
          <li>
            <strong>Attention Heatmap Dashboard.</strong> A timed tracking task
            logs cursor or gaze events, awards points for hits, and renders a
            cumulative heatmap overlay.
          </li>
          <li>
            <strong>Adaptive Bandwidth Controller.</strong> Facilitators can
            simulate network conditions, toggle adaptive streaming, and inspect
            bandwidth usage alongside availability.
          </li>
        </ul>
        <p><strong>Key characteristics:</strong></p>
        <ul>
          <li>
            Self-contained in <code>index.html</code> with no build or backend
            step.
          </li>
          <li>
            Right-side drawer centralises toggles, sliders, and diagnostics.
          </li>
          <li>
            Hero landing screen doubles as onboarding copy and collapses to
            maximise the stage.
          </li>
          <li>
            Accessibility notes include keyboard-accessible controls, ARIA
            labels, and stateful badges for adaptive modes.
          </li>
        </ul>

        <figure class="overview-figure">
          <img
            src="images/computer%20prototype%20screenshot.png"
            alt="Computer prototype interface showing the gaze-adaptive player, task overview, and unified control drawer."
          />
          <figcaption>
            High-fidelity computer prototype highlighting the hero onboarding
            panel (left) and multi-tool control drawer (right).
          </figcaption>
        </figure>

        <h3>1.2 User Goals &amp; Scenarios</h3>
        <ul>
          <li>
            <strong>Faculty presenters</strong> demonstrate adaptive streaming
            to prospective partners, quickly enabling or disabling spotlight and
            bandwidth features during live walkthroughs.
          </li>
          <li>
            <strong>HCI researchers</strong> capture gaze/cursor events while
            participants complete timed tracking tasks, exporting heatmap states
            for follow-up analysis.
          </li>
          <li>
            <strong>Technical facilitators</strong> stress-test network states
            in the same interface, correlating playback quality with simulated
            bandwidth dips.
          </li>
        </ul>
        <p>
          Each scenario starts with the hero overview, allowing the facilitator
          to brief newcomers before collapsing the panel and focusing on the
          active module. The consistent drawer layout reinforces muscle memory
          so users do not have to hunt for controls as they switch context.
        </p>

        <h3>1.3 Current Evaluation Focus</h3>
        <p>
          Recent usability sessions examine how quickly observers identify the
          spotlight’s effect, whether the heatmap hints aid situational
          awareness, and how intuitively facilitators notice bandwidth
          bottlenecks. Metrics include time-to-first-success for each task,
          perceived clarity of status badges, and the number of prompts required
          to locate key toggles.
        </p>
        <p>
          These insights feed the iteration backlog—for example, tweaking badge
          contrast, refining tooltip copy, and smoothing the transition when the
          hero panel collapses into the top navigation strip.
        </p>

        <h3>1.4 Design Evolution (with rationale)</h3>
        <table>
          <thead>
            <tr>
              <th>No.</th>
              <th>Component / Change</th>
              <th>Why We Changed It</th>
              <th>Usability / Feasibility Rationale</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td><strong>Single-page architecture with spotlight toggle surfaced in Task 1</strong></td>
              <td>
                Testers were confused by multiple HTML entry points ("Which file do I open?")
                and often missed the original spotlight toggle buried in a sub-menu. Peers
                suggested converging on one launch path.
              </td>
              <td>
                One <code>index.html</code> reduces onboarding friction and keeps the key
                adaptive control in immediate view. This supports faster learnability (UQ1)
                and simplifies deployment since all scripts run in a single document.
              </td>
            </tr>
            <tr>
              <td>2</td>
              <td><strong>Hero overview cards plus hide/show toggle</strong></td>
              <td>Participants loved the contextual cards but said they blocked demos and needed a quick collapse option.</td>
              <td>The <code>Hide task overview</code> button delivers just-in-time onboarding and frees the stage on demand, improving visibility without sacrificing discoverability for new users.</td>
            </tr>
            <tr>
              <td>3</td>
              <td><strong>Unified right-hand drawer integrating spotlight, heatmap, and bandwidth controls</strong></td>
              <td>Early builds scattered controls across tabs. Low-fi sessions showed users wanted "all knobs in one place" and facilitators needed quick diagnostics.</td>
              <td>Consolidating controls shortens task switching, reduces navigation errors, and keeps instrumentation (bandwidth toggle, status badges) manageable within the layout.</td>
            </tr>
          </tbody>
        </table>
        <p><em>Table 1. Principal design changes enacted after low-fidelity testing.</em></p>

        <h4>1.4.1 Detailed Rationale</h4>
        <ol>
          <li>
            <p>
              <strong>From multi-file prototype to a single-page suite.</strong>
              Three of five lo-fi participants hesitated at the file tree, unsure which HTML
              file to open, and facilitators worried about keeping tabs synchronised. We
              explored adding a launcher page that linked to task-specific files, but it
              introduced an extra step and duplicated JavaScript. Merging into
              <code>index.html</code> with the spotlight toggle surfaced at the top of the
              drawer reduces cognitive load and enables shared state across tasks (heatmaps,
              diagnostics) that would be harder to synchronise otherwise.
            </p>
          </li>
          <li>
            <p>
              <strong>Hero overview with contextual toggle.</strong>
              Reviewers appreciated the narrative cards yet wanted them out of the way once
              testing started. Keeping them always visible hid the stage, but removing them
              entirely harmed onboarding. The hero-level toggle now adds or removes a
              <code>task-summary-hidden</code> class, working responsively and respecting
              reduced-motion preferences so the experience stays accessible.
            </p>
          </li>
          <li>
            <p>
              <strong>Unifying task controls in a persistent drawer.</strong>
              Observers repeatedly asked where to adjust the blur radius or find the
              bandwidth panel when controls were scattered. Centralising the controls improves
              recognition, keeps keyboard navigation predictable, and allowed us to refactor
              task state objects (<code>focusState</code>, <code>trackingTask</code>,
              <code>adaptiveController</code>) into one module without global collisions.
            </p>
          </li>
        </ol>

        <h4>1.4.2 Supporting Adjustments</h4>
        <ul>
          <li>
            <strong>Bandwidth monitor toggle and status pill.</strong> Added to keep the
            overlay from occluding the scene while giving facilitators quick access to
            diagnostics.
          </li>
          <li>
            <strong>Task HUD refinement.</strong> Made score and timer feedback persistent so
            observers maintain situational awareness during the tracking exercise.
          </li>
          <li>
            <strong>Adaptive encoding badge repositioning.</strong> Shifted left to avoid
            collisions with the focus settings button, resolving an occlusion issue spotted in
            hallway tests.
          </li>
        </ul>
        <p>
          Collectively, these updates address the pain points surfaced in lo-fi sessions:
          uncertain entry points, obstructed staging area, and dispersed controls. The
          high-fidelity prototype is now easier to demo, faster to learn, and better aligned
          with the feasibility constraints of a single-page deployment.
        </p>

        <h3>1.5 Prototype Implementation</h3>
        <p>
          The walkthrough below captures the implemented prototype in action, highlighting the
          unified drawer, adaptive focus effect, and task transitions that evaluators experience
          during usability sessions.
        </p>
        <div class="video-wrapper">
          <video
            controls
            preload="metadata"
            poster="images/computer%20prototype%20screenshot.png"
            aria-label="Demo video of the gaze-aware computer prototype"
          >
            <source src="videos/video1.mp4" type="video/mp4" />
            Your browser does not support embedded video. You can
            <a href="videos/video1.mp4">download the MP4 file</a> instead.
          </video>
        </div>
      </section>

      <section>
        <h2>2. User Manual &amp; Installation Guide</h2>
        <div class="pdf-grid">
          <div class="pdf-card">
            <h3>2.1 User Guide</h3>
            <p>
              Step-by-step instructions covering the core streaming experience,
              adaptive gaze features, and troubleshooting tips.
            </p>
            <a href="files/userguide.pdf" target="_blank" rel="noopener">
              Open in a new tab
            </a>
            <iframe
              class="pdf-embed"
              src="files/userguide.pdf#toolbar=0&navpanes=0&scrollbar=0"
              title="Gaze-aware streaming user guide"
            >
            </iframe>
          </div>
          <div class="pdf-card">
            <h3>2.2 Installation Guide</h3>
            <p>
              Environment requirements, setup checklist, and deployment notes
              for running the computer prototype locally.
            </p>
            <a href="files/install.pdf" target="_blank" rel="noopener">
              Open in a new tab
            </a>
            <iframe
              class="pdf-embed"
              src="files/install.pdf#toolbar=0&navpanes=0&scrollbar=0"
              title="Gaze-aware streaming installation guide"
            >
            </iframe>
          </div>
        </div>
      </section>

      <section>
        <h2>3. Usability Test Plan</h2>

        <h3>3.1 User Population</h3>
        <p>The intended sample for the next round of testing is summarised below.</p>
        <table>
          <thead>
            <tr>
              <th>Attribute</th>
              <th>Definition</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Target users</td>
              <td>
                Knowledge workers and graduate students who regularly consume remote
                presentations, training clips, or screen-share walkthroughs (aligned
                with our original design brief).
              </td>
            </tr>
            <tr>
              <td>Sample size</td>
              <td>3&ndash;5 participants per round.</td>
            </tr>
            <tr>
              <td>Background</td>
              <td>
                Comfortable with web browsers, mouse/trackpad navigation, and
                multitasking between content and controls.
              </td>
            </tr>
            <tr>
              <td>Usage context</td>
              <td>
                Watching adaptive media on laptops/desktop monitors in quiet
                office or home-office settings, occasionally on shared screens
                during team workshops.
              </td>
            </tr>
          </tbody>
        </table>

        <h3>3.2 Usability Goals</h3>
        <table>
          <thead>
            <tr>
              <th>ID</th>
              <th>Usability Goal</th>
              <th>Quantitative Measurement (where appropriate)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>UQ1</td>
              <td>
                <strong>Learnability</strong> &mdash; Users understand how to
                activate and use each adaptive mode (focus blur, speed
                adaptation, bandwidth control) without external guidance.
              </td>
              <td>
                <ul>
                  <li>&ge; 90% of users locate and toggle the adaptive switch within 30 s.</li>
                  <li>First complete run of any task finishes within 2 min without help.</li>
                </ul>
              </td>
            </tr>
            <tr>
              <td>UQ2</td>
              <td>
                <strong>Efficiency</strong> &mdash; Adaptive mode helps users
                complete visual search tasks faster and with fewer cursor
                movements.
              </td>
              <td>
                <ul>
                  <li>Target acquisition time (adaptive ON) &le; 80% of adaptive OFF.</li>
                  <li>Cursor travel distance per hit reduces by &ge; 15%.</li>
                </ul>
              </td>
            </tr>
            <tr>
              <td>UQ3</td>
              <td>
                <strong>Satisfaction</strong> &mdash; Adaptive effects feel
                comfortable and visually pleasant.
              </td>
              <td>
                <ul>
                  <li>&ge; 80% of participants rate comfort, clarity, and visual pleasantness &ge; 4/5.</li>
                </ul>
              </td>
            </tr>
            <tr>
              <td>UQ4</td>
              <td>
                <strong>Perceived Clarity &amp; Focus</strong> &mdash; Adaptive
                blur helps users see important regions more clearly.
              </td>
              <td>
                <ul>
                  <li>&ge; 75% of users report improved clarity in adaptive ON mode.</li>
                  <li>Cursor-based heatmaps show &ge; 25% higher fixation density on target zones.</li>
                </ul>
              </td>
            </tr>
            <tr>
              <td>UQ5</td>
              <td>
                <strong>Robustness &amp; Stability</strong> &mdash; The adaptive
                system remains visually stable under network fluctuation.
              </td>
              <td>
                <ul>
                  <li>Latency &lt; 80 ms during adaptive updates.</li>
                  <li>&ge; 95% of bandwidth tests show no visible flicker or contrast loss.</li>
                </ul>
              </td>
            </tr>
            <tr>
              <td>UQ6</td>
              <td>
                <strong>Awareness &amp; Control</strong> &mdash; Users feel aware
                of adaptive changes and perceive them as helpful rather than
                distracting.
              </td>
              <td>
                <ul>
                  <li>&ge; 80% of users report feeling in control.</li>
                  <li>&lt; 10% describe adaptive effects as distracting or confusing.</li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

        <h3>3.3 Test Procedure</h3>
        <h4>Number of Examiners</h4>
        <p>
          Three examiners participate in each usability session to ensure
          consistent, objective observation and complete data capture.
        </p>
        <ul>
          <li>
            <strong>Facilitator.</strong> Welcomes participants, explains the
            purpose of the session, and guides them through each task following
            a standardised script. Introduces the think-aloud process and
            maintains a neutral tone throughout the test.
          </li>
          <li>
            <strong>Observer.</strong> Watches silently, recording visible
            difficulties, navigation behaviour, pauses, and comments to identify
            usability barriers and user reactions.
          </li>
          <li>
            <strong>Recorder.</strong> Measures task duration, manages screen
            and audio recordings, and ensures that all timing and data logs are
            accurate.
          </li>
        </ul>
        <p>
          This setup ensures that both quantitative and qualitative data are
          collected without overloading any single examiner.
        </p>

        <h4>Equipment Needed</h4>
        <p>The following equipment and materials are prepared for each test:</p>
        <ul>
          <li>
            Laptop or desktop computer with Chrome, Edge, or Firefox installed.
          </li>
          <li>
            Prototype HTML files (<code>task1_focus.html</code>,
            <code>task2_search.html</code>, <code>task3_bandwidth.html</code>)
            which are integrated into one file<code>index.html</code>, <opened through a local server using
            <code>python -m http.server 8000</code>.
          </li>
          <li>Screen recording software (e.g., OBS Studio).</li>
          <li>Audio recorder to capture think-aloud comments.</li>
          <li>Stopwatch or timing tool for manual verification.</li>
          <li>
            Printed consent form, facilitator script, pre-/post-test
            questionnaires, and data collection sheets.
          </li>
        </ul>
        <p>
          Before each session, examiners verify that all prototypes open
          correctly, adaptive ON/OFF toggles function, and recordings work
          properly.
        </p>

        <h4>Handling of the Prototype</h4>
        <ul>
          <li>The prototypes run in the browser and should stay in full-screen mode.</li>
          <li>The facilitator opens the local server and navigates to each task in sequence.</li>
          <li>Each task receives only a brief introduction to avoid bias.</li>
          <li>No code or parameters are changed during testing.</li>
          <li>Participants interact naturally using the mouse.</li>
          <li>Provide 1&ndash;2 minute breaks between tasks and debrief at the end.</li>
        </ul>

        <h4>Instructions for Examiners</h4>
        <p>
          Each examiner plays a defined role to keep sessions smooth, unbiased,
          and well-documented. Close coordination preserves standardisation
          across all participants.
        </p>
        <p>
          <strong>Facilitator:</strong> The facilitator leads the session and 
          interacts directly with the participant. Their primary responsibility
          is to explain the purpose of the study, clarify the procedure, and make 
          sure the participant feels comfortable before testing begins. The
          facilitator introduces the three prototype tasks which are adaptive focus
          mode, heatmap visual search, and bandwidth control, emphasizing that the 
          purpose of the session is to evaluate the system’s usability, not the 
          participant’s ability. Before starting, the facilitator obtains informed 
          consent and introduces the think-aloud protocol, asking participants to 
          verbalize what they notice, what confuses them, or what they find clear 
          when the adaptive mode switches ON or OFF. During the test, the facilitator 
          provides instructions one task at a time, avoids offering feedback or hints, 
          and stays neutral in tone and expression. At the end of the session, the 
          facilitator thanks the participant and asks short follow-up questions to 
          collect subjective impressions of clarity, comfort, and task difficulty.
        </p>
        <p>
          <strong>Observer:</strong> The observer is responsible for documenting 
          the participant’s visible behaviors, such as hesitation, confusion, or 
          eye and cursor movements. They record how users react to adaptive blur
          changes, where the cursor tends to move, and whether the participant 
          takes longer to find targets when adaptive mode is OFF. The observer 
          remains silent and seated away from the screen to minimize influence, 
          taking timestamped notes on errors, pauses, and verbal comments. These 
          notes serve as the main qualitative record of user behavior and will be 
          synchronized with screen recordings after each session.
        </p>
        <p>
          <strong>Recorder:</strong> The recorder manages all timing and data 
          collection equipment. They record task start and end times, measure completion 
          duration for each adaptive condition, and ensure that all video and audio
          recordings are functioning properly. The recorder also checks the Python 
          local server and browser to make sure the prototype files (<code>task1.html,</code> 
          <code>task2.html,</code> and <code>task3.html</code>)
          load correctly before the session begins. They remain silent during testing 
          but stay alert for any technical issues, such as frozen screens or recording 
          failures, which must be addressed immediately without disturbing the participant.
        </p>
        <p>
          This structured division of responsibilities ensures that every usability test
          remains consistent, neutral, and well-documented. By clearly defining each 
          examiner’s duties, the team ensures reliable data collection, combining quantitative
          measurements such as task completion time and error rate with qualitative insights
          from user feedback and recorded observations.
        </p>

        <h4>Treatment of Participants</h4>
      <p>
          All participants will be treated with professionalism, respect, and care throughout
          the usability testing process. Examiners are responsible for creating a calm and 
          supportive environment in which participants feel comfortable exploring the adaptive 
          prototypes freely. Each participant will be reminded that the purpose of the test is 
          to evaluate the interface design, not their personal performance or skill.
            </p>
          <p>
            During the session, participants will be encouraged to verbalize their thoughts 
            and reactions using the think-aloud protocol. They may describe what they notice 
            when adaptive blur changes, how their focus shifts during the heatmap task, or 
            how network adjustments affect the clarity of the interface. This feedback will
            help the team understand both intuitive and confusing elements of each adaptive mode.
          </p>
         <p>
            Before testing begins, informed consent will be obtained for all forms of recording, 
            including screen, video, and audio capture. Participants will be clearly informed 
            about what data is collected, how it will be stored, and that all results will remain 
            anonymous. They have the right to pause, stop, or withdraw from the test at any time, 
            or to request deletion of any recorded material without any penalty. Examiners will 
            maintain a neutral and friendly attitude, avoiding any expressions that may influence 
            the participant’s behavior or confidence.
         </p>

        <h4>What Examiners Should Avoid</h4>
        <p>
          Examiners must avoid any behavior that could bias the test results or influence 
          how participants interact with the adaptive system. They should not provide hints, 
          confirm correctness, or explain how each adaptive mode works during the session. 
          Examiners must refrain from giving feedback such as “good,” “correct,” or “almost,”
          and avoid leading gestures or eye contact that suggest expected actions.
        </p>
        <p>
          During the focus, heatmap, and bandwidth tasks, examiners must not intervene to 
          correct cursor position, point out visual targets, or clarify adaptive effects, as 
          this could compromise the authenticity of user reactions. They should remain silent
          and avoid commenting on errors or confusion, ensuring that the data collected truly 
          reflects natural user behavior. These precautions guarantee that both performance 
          data (e.g., task time, hit accuracy) and user perceptions remain unbiased, providing 
          valid and reliable insights into the usability of each adaptive feature.
        </p>

        <h4>Measurements: What, How, and When</h4>
        <p>
          Measurements will be collected during and after each task to evaluate user performance, 
          accuracy, and perception of the adaptive visual system. Quantitative data will provide 
          objective evidence of efficiency, while qualitative data will capture users’ impressions 
          and perceived clarity. Each measurement is associated with a specific collection method 
          to ensure both consistency and reliability.
        </p>
        <table>
          <thead>
            <tr>
              <th>Measurement</th>
              <th>Description</th>
              <th>Collection Method</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Task Completion Time</td>
              <td>
                Time to complete each visual search or interaction task under
                adaptive ON and OFF modes.
              </td>
              <td>Built-in timer plus screen recording.</td>
            </tr>
            <tr>
              <td>Cursor Travel Distance</td>
              <td>
                Total cursor distance between targets, reflecting efficiency and
                precision.
              </td>
              <td>Logged via internal coordinate tracking script.</td>
            </tr>
            <tr>
              <td>Error Count</td>
              <td>
                Missed targets, inaccurate clicks, or premature selections for
                each task.
              </td>
              <td>Observer notes verified against interaction logs.</td>
            </tr>
            <tr>
              <td>Heatmap Density</td>
              <td>
                Distribution of cursor positions to visualise attention focus
                and scanning behaviour.
              </td>
              <td>Automatically generated heatmap PNG after each trial.</td>
            </tr>
            <tr>
              <td>Post-test Satisfaction &amp; Clarity Ratings</td>
              <td>
                Participant-reported comfort, clarity, and visual stability when
                comparing adaptive ON and OFF modes.
              </td>
              <td>Five-point Likert questionnaire after each session.</td>
            </tr>
          </tbody>
        </table>

        <h4>Evaluation Context</h4>
        <p>
          Each usability session is designed to last approximately 25 to 30 minutes, 
          consisting of a 5-minute briefing and consent process, 20 minutes of adaptive 
          task interaction, and a 5-minute post-test discussion and questionnaire. 
          Participants will be informed of the study’s purpose and recording procedures 
          before testing begins, and consent will be obtained for all screen and data 
          captures.
          <li>
            All tasks are performed on a desktop computer running the FocusVision prototype.
          </li>
        </p>
        <ul>
          <li>
            <strong>Task 1 &mdash; Speed-Sensitive Cursor Blur:</strong>
            Participants move the cursor at varying speeds to observe adaptive focus changes, 
            testing comfort and naturalness of motion.
          </li>
          <li>
            <strong>Task 2 &mdash; Attention Heatmap Search:</strong> The participants first create a 
            heat map by concentrating on the object of interest. We assume that the machine will
            analyze the object of interest. After that, participants
            locate randomised date labels while heatmaps record cursor paths to
            compare adaptive ON and OFF performance.
          </li>
          <li>
            <strong>Task 3 &mdash; Adaptive Bandwidth:</strong> Participants view a simulated 
            video-conference interface where network fluctuation dynamically adjusts visual 
            clarity, evaluating the perceived smoothness and system responsiveness.
          </li>
        </ul>
        <p>
          The test environment records both quantitative metrics—such as task time, cursor distance, 
          and error rate—and qualitative indicators like comments and visible hesitation. Examiners 
          observe user behavior discreetly and note moments of confusion or adjustment. After all 
          tasks, participants complete a short questionnaire rating comfort, clarity, and focus stability.
          <li>
            This mixed-method evaluation provides balanced insights into both measured performance and 
            subjective perception. By combining objective data logging with direct participant 
            feedback, the study verifies how adaptive ON mode enhances efficiency, reduces distraction, 
            and supports clearer visual interaction compared with the static OFF condition.
          </li>
        </p>
        
        <h3>3.4 Reporting: Adaptive Display System Testing Plan</h3>
        <p>
          This plan explains how examiners will record and report data from the usability tests of the 
          adaptive display prototype, ensuring results are clear and comparable for later analysis.
        </p>
        <h4>Participants</h4>
        <p>
          Four to six university students or young professionals complete the
          test using desktop or laptop computers with Chrome or Edge in a quiet
          room while working through three adaptive display tasks.
        </p>
        <h4>Data to Record</h4>
        <p><strong>Quantitative data:</strong></p>
        <ul>
          <li>Task completion time.</li>
          <li>Number of cursor hits on target (Task 2).</li>
          <li>Average cursor speed.</li>
          <li>Heatmap fixation density.</li>
          <li>Bandwidth recovery delay (Task 3).</li>
          <li>Task success rate.</li>
        </ul>
        <p><strong>Qualitative data:</strong></p>
        <ul>
          <li>Participant comments and think-aloud feedback.</li>
          <li>Observed confusion, hesitation, or satisfaction.</li>
          <li>Post-test reflections on clarity and comfort.</li>
        </ul>
        <h4>Recording and Reporting</h4>
        <ul>
          <li>
            Screen-record every session and capture observer notes with
            timestamps.
          </li>
          <li>Summarise task durations and success counts.</li>
          <li>Include generated heatmaps and bandwidth graphs.</li>
          <li>
            Highlight key participant quotes and usability ratings on the 1&ndash;5
            scale.
          </li>
        </ul>
        <p>
          All results will be compiled into a single report with both numerical data and qualitative 
          notes. Quantitative data will be compared between adaptive ON and OFF modes, and qualitative
          insights will be grouped by theme (clarity, effort, focus).
        </p>
        <h4>Design Implications</h4>
        <p>
          The data will help evaluate whether adaptive effects improve focus and visual comfort. 
          Faster target finding, smoother transitions, and higher fixation in adaptive ON mode will 
          indicate success and guide further tuning of blur and latency parameters.
        </p>
      </section>

      <section>
        <h2>4. Usability Evaluation Plan</h2>

        <h3>4.1 Purpose and Objectives</h3>
        <p>
          The purpose of this evaluation is to examine the usability, responsiveness, and visual 
          clarity of the gaze-adaptive prototype, which consists of three tasks designed to explore 
          user attention and visual comfort. Independent evaluators will assess whether the
          system supports focused visual interaction without causing distraction or confusion.
        </p>
        <p>The evaluation investigates:</p>
        <ul>
          <li>Effectiveness of adaptive blur and focus mechanisms in guiding attention.</li>
          <li>Intuitiveness of switching between adaptive ON and OFF modes.</li>
          <li>
            Clarity of visual cues, timing feedback, and exported results such as
            heatmaps and speed plots.
          </li>
        </ul>
        <p>
          The process confirms that the interface remains usable, understandable,
          and visually comfortable for both first-time and experienced users.
        </p>

        <h3>4.2 Evaluation Methodology</h3>
        <p>
          The methodology follows heuristic evaluation principles adapted from Nielsen’s usability 
          heuristics, supplemented with criteria relevant to adaptive visual systems. Each 
          evaluator will use the prototype independently, record usability issues, and rate their 
          severity using a standardized 0–4 scale (0 = not an issue, 4 = critical).
        </p>
        <h4>Selected Heuristics</h4>
        <ul>
          <li>
            <strong>Visibility of System Status:</strong> The system should clearly 
            indicate whether Adaptive ON or OFF is active. Users must be able to perceive 
            focus blur transitions and overall visual state changes instantly.
          </li>
          <li>
            <strong>User Control and Freedom:</strong> Users should easily toggle 
            modes, start or end tests, and download results without confusion
            or unexpected resets.
          </li>
          <li>
            <strong>Consistency and Standards:</strong> Button layout, labeling (e.g., Start
            Test, Download Heatmap), and adaptive feedback should follow consistent 
            placement and behavior across tasks.
          </li>
          <li>
            <strong>Minimal Intrusion and Flow Balance:</strong> The interface should 
            maintain calmness—using smooth transitions, soft contrast,
            and no harsh prompts—so that users 
            remain comfortable during long tests.
          </li>
          <li>
            <strong>Aesthetic and Emotional Tone:</strong> Preserve calm,
            smooth transitions, gentle contrast, and avoid harsh prompts to
            support comfort.
          </li>
          <li>
            <strong>Feedback and Clarity of Results:</strong> Heatmaps and 
            speed plots should provide accurate, immediate feedback that 
            users can interpret without technical explanation.
          </li>
        </ul>

        <h3>4.3 Evaluation Procedure</h3>
        <ul style="list-style-type:none; padding-left:0;">
          <li>
            <strong>Independent Exploration:</strong> Each evaluator completes at
            least two runs per task (adaptive OFF and ON), noting confusion,
            unexpected blur transitions, or unclear status indicators.
          </li>
          <li>
            <strong>Heuristic Logging:</strong> 
            <ul style="list-style-type:none; padding-left:1.5em;">
            <li>For each heuristic, evaluators record:</li>
              <li>•	Description of the usability issue.</li>
              <li>•	Screenshot or timestamp (if relevant).</li>
              <li>•	Suggested improvement.</li>
              <li>•	Severity rating (0–4).</li>
        </ul>
          </li>
          <li>
            <strong>Team Review and Consensus:</strong> After independent sessions, 
            evaluators will meet to consolidate findings and assign a final severity 
            rating to each identified problem.
          </li>
          <li>
            <strong>Reporting:</strong> Produce a summary that will include:A table of all 
            identified usability issues, grouped by heuristic; Representative screenshots 
            or brief notes; Ranked severity and recommended design actions
            (retain / revise / remove); The compiled report will guide improvements
            in the next design iteration.
          </li>
        </ul>
        

        <h3>4.4 Severity Rating Scale</h3>
        <table>
          <thead>
            <tr>
              <th>Rating</th>
              <th>Meaning</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>0</td>
              <td>Not a usability issue.</td>
            </tr>
            <tr>
              <td>1</td>
              <td>Cosmetic issue (optional fix).</td>
            </tr>
            <tr>
              <td>2</td>
              <td>Minor issue (low priority).</td>
            </tr>
            <tr>
              <td>3</td>
              <td>Major issue (significant impact).</td>
            </tr>
            <tr>
              <td>4</td>
              <td>Critical issue (must fix before next test).</td>
            </tr>
          </tbody>
        </table>

        <h3>4.5 Expected Outcomes</h3>
        <p>
          The evaluation will determine whether the adaptive gaze mechanism and mode 
          switching improve user focus, comfort, and efficiency compared to static presentation.
           Findings will provide concrete guidance for refining visual timing, blur intensity, and 
          feedback presentation to achieve an optimal balance between immersion and awareness.

        </p>
      </section>
    </div>
  </body>
</html>
