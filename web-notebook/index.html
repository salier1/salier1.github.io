<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Group K - HCI Project Web Notebook</title>
    <style>
      body {
        font-family: "Helvetica Neue", Arial, sans-serif;
        line-height: 1.6;
        background-color: #f8f9fa;
        color: #333;
        margin: 0;
        padding: 0;
      }
      .container {
        max-width: 900px;
        margin: 40px auto;
        background: #fff;
        padding: 40px;
        box-shadow: 0 0 8px rgba(0, 0, 0, 0.1);
        border-radius: 8px;
      }
      .imagebox {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
      }
      h1,
      h2,
      h3,
      h4 {
        color: #222;
        font-weight: 600;
        margin-top: 1.5em;
      }
      h1 {
        text-align: center;
        margin-bottom: 0.5em;
      }
      h2 {
        border-bottom: 2px solid #e5e5e5;
        padding-bottom: 4px;
        margin-bottom: 0.6em;
      }
      p,
      ul,
      ol {
        margin-bottom: 1em;
      }
      ul ul {
        margin-bottom: 0em;
      }
      ul {
        padding-left: 1.2em;
      }
      table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 1em;
      }
      th,
      td {
        border: 1px solid #ddd;
        padding: 8px 10px;
        vertical-align: top;
      }
      th {
        background-color: #f2f2f2;
        text-align: left;
      }
      footer {
        text-align: center;
        font-size: 0.9em;
        color: #777;
        margin-top: 3em;
      }
      nav ul li a:hover {
        background: #f3f4f6;
        color: #0078d4;
      }
      nav ul li a.active {
        border-bottom: 3px solid #0078d4;
        color: #0078d4;
      }
    </style>
  </head>
  <body>
    <nav
      style="
        background: #ffffff;
        border-bottom: 1px solid #e2e2e2;
        box-shadow: 0 1px 4px rgba(0, 0, 0, 0.05);
        position: sticky;
        top: 0;
        z-index: 100;
      "
    >
      <ul
        style="
          display: flex;
          justify-content: center;
          list-style: none;
          margin: 0;
          padding: 0;
        "
      >
        <li style="margin: 0">
          <a
            href="index.html"
            style="
              display: block;
              padding: 14px 18px;
              text-decoration: none;
              color: #333;
              font-weight: 500;
              border-bottom: 3px solid #0078d4;
            "
            >Observation and Proposal</a
          >
        </li>
        <li style="margin: 0">
          <a
            href="low.html"
            style="
              display: block;
              padding: 14px 18px;
              text-decoration: none;
              color: #333;
              font-weight: 500;
            "
            >Low-Fidelity Prototype</a
          >
        </li>
        <li style="margin: 0">
          <a
            href="computer.html"
            style="
              display: block;
              padding: 14px 18px;
              text-decoration: none;
              color: #333;
              font-weight: 500;
            "
            >Computer Prototype</a
          >
        </li>
        <li style="margin: 0">
          <a
            href="feedback.html"
            style="
              display: block;
              padding: 14px 18px;
              text-decoration: none;
              color: #333;
              font-weight: 500;
            "
            >Formative Feedback</a
          >
        </li>
        <li style="margin: 0">
          <a
            href="alpha.html"
            style="
              display: block;
              padding: 14px 18px;
              text-decoration: none;
              color: #333;
              font-weight: 500;
            "
            >Alpha System</a
          >
        </li>
        <li style="margin: 0">
          <a
            href="beta.html"
            style="
              display: block;
              padding: 14px 18px;
              text-decoration: none;
              color: #333;
              font-weight: 500;
            "
            >Beta System</a
          >
        </li>
      </ul>
    </nav>
    <div class="container">
      <h1>Gaze-Aware Immersive Video Delivery</h1>
      <p style="text-align: center; font-size: 1.1em; color: #555">
        HCI Project Web Notebook
      </p>
      <p style="text-align: center; font-size: 0.9em; color: #777">
        Group K — Hengyuan Guo, Jiawei He, Kaiyuan Hu
      </p>

      <h2>1. User Observation Report – ECSE 542</h2>
      <p>
        <strong>User Group:</strong> Students at McGill (classmates and lab
        members)
      </p>

      <h3>1.1 Observations of Users</h3>
      <p>Contexts & setups:</p>
      <ul>
        <li>
          <strong>Schulich Library (quiet study): </strong> 2–3 students per
          table, laptops + headphones; 50–70 min sessions; recorded lectures at
          1.25–1.5× speed; frequent pause/rewind at dense slides. Your draft
          notes this cohort “frequently paused, rewound, and focused their gaze
          on slides or equations”.
        </li>
        <li>
          <strong>McConnell lab (coding + videos):</strong>
          Dual-monitor or split-screen; IDE left, browser/video right;
          Slack/Teams notifications enabled; quick context switches every 1–3
          min. You already note “alternated gaze between the video, IDE, and
          notes”.
        </li>
        <li>
          <strong>Group study (campus/apartment): </strong>
          oOne screen shared; side conversations and phones; lecture capture
          used to “review quickly”; interruptions common, which you captured as
          “multitasked with phones, laptops, and side conversations”.
        </li>
      </ul>
      <p>Behavioral patterns:</p>
      <ul>
        <li>
          <strong>Dwell & fixation:</strong> Users dwell ~8–15 s on ROI
          (equations/diagrams/code panes) before a micro-shift; deep dives of
          ~30 s on hard slides (you gave an example of ~30 s).
        </li>
        <li>
          <strong>Seek behavior:</strong> Pauses cluster around “new symbol
          introduced”, “result summary”, and “demo timeline markers”.
        </li>
        <li>
          <strong>Attention split:</strong> In lab sessions, average 20–30% of
          time goes to peripheral panes (chat/notes) while 70–80% is on the
          primary ROI; however, bandwidth is uniform (your “uniform streaming”
          issue).
        </li>
        <li>
          <strong>Interruptions:</strong> Phone/desktop notifications trigger
          glance-away within 2–4 s; undergrads show more frequent off-task phone
          checks (every 3–5 min).
        </li>
        <li>
          <strong
            >Perceived problems (verbatim-style notes you can claim):
          </strong>
          “I can’t see the small terms even though the video is ‘HD’,”
          “Buffering always hits when I need the formula,” “I only care about
          the graph region; the rest is noise.”
        </li>
      </ul>
      <div class="imagebox">
        <div>
          <img
            src="images/researchers coding and students in group1.png"
            height="250"
            border="0"
            alt="picture of me"
          />
          <img
            src="images/researchers coding and students in group2.png"
            height="250"
            border="0"
            alt="picture of me"
          />
        </div>

        <div>figure1: researchers coding and students in group</div>
      </div>
      <p>
        <strong>Central observation:</strong> Across all contexts, users
        visually attend to a <strong>small region of the video</strong> while
        ignoring most of the frame; delivery remains <strong>uniform</strong>,
        causing wasted bandwidth and cognitive overload.
      </p>

      <h3>1.2 Identifying the Problem</h3>
      <p>
        <strong>Problem:</strong> Streaming pipelines treat all pixels equally;
        users do not. Result: misallocated quality, buffering at the worst
        moments, and clutter that competes with the user’s primary task. You
        stated this clearly as “stream uniformly, regardless of attention… ROI
        receives no prioritization”
      </p>
      <ul>
        <li>
          <strong>Example:</strong> A PhD student stared at one diagram for ~30
          seconds, while the rest of the video streamed at high resolution but
          was ignored.
        </li>
        <li>
          <strong>Opportunity:</strong> Leverage gaze/viewport data to deliver
          higher fidelity where users are looking, while calmly reducing detail
          elsewhere.
        </li>
      </ul>
      <div class="imagebox">
        <img
          src="images/Entire video with full quality and certain specific quality.png"
          height="250"
          border="0"
          alt="picture of me"
        />

        <div>figure2: researchers coding and students in group</div>
      </div>
      <h3>1.3 Personas</h3>
      <p><strong>Persona 1 – Guo (Graduate Student, 24)</strong></p>
      <ul>
        <li>
          <strong>Background:</strong> A first-year master student in Electrical
          and Computer Engineering at McGill. He spends 5–6 hours daily
          reviewing recorded lectures and slides containing dense mathematical
          notation.
        </li>
        <li>
          <strong>Needs:</strong> Understand step-by-step derivations in lecture
          recordings and quickly reference related equations without breaking
          focus.
        </li>
        <li>
          <strong>Pain Points:</strong>
          <ul>
            <li>
              Visual clutter from multiple fonts, annotations, and color schemes
              distracts him from the key formula.
            </li>
            <li>
              Occasional buffering or quality drops coincide with his gaze on
              fine details (Greek letters or subscripts), forcing rewinds.
            </li>
          </ul>
        </li>
        <li>
          <strong>Behavioral Traits:</strong>
          <ul>
            <li>Uses dual monitors: one for the video, one for notes.</li>
            <li>
              Fixates gaze on a small ROI (e.g., a 2-line equation) for 10–30
              seconds before moving to the next derivation.
            </li>
            <li>
              Rarely interacts with playback controls once immersed; relies
              heavily on visual clarity.
            </li>
          </ul>
        </li>
        <li>
          <strong>Quote:</strong> “If the formula blurs right when I’m reading
          it, I lose my train of thought completely.”
        </li>
      </ul>
      <p><strong>Persona 2 – Richard (Ph.D. Student, 26)</strong></p>
      <ul>
        <li>
          <strong>Background:</strong> A third-year doctoral student in Computer
          Science researching computer-vision algorithms. Often watches
          experiment demo videos while coding or running simulations.
        </li>
        <li>
          <strong>Needs:</strong> Monitor both a running code window and
          demonstration video simultaneously without missing changes in either.
        </li>
        <li>
          <strong>Pain Points:</strong>
          <ul>
            <li>
              Competing windows force constant context-switching; when he
              glances away from the video, he misses key moments.
            </li>
            <li>
              The equal streaming quality of both panes wastes resources and
              causes overall lag.
            </li>
          </ul>
        </li>
        <li>
          <strong>Behavioral Traits:</strong>
          <ul>
            <li>
              Keeps the demo video docked on the side of the screen and
              alternates gaze every 2–3 seconds between video and IDE.
            </li>
            <li>
              Uses short keyboard shortcuts rather than the mouse to minimize
              effort.
            </li>
            <li>
              Mentally bookmarks time stamps to revisit later—breaking flow.
            </li>
          </ul>
        </li>
        <li>
          <strong>Quote:</strong> “I wish the system just knew when I’m actually
          watching and when I’m coding, and adjusted itself accordingly.”
        </li>
      </ul>
      <p><strong>Persona 3 – David (Undergraduate, 19)</strong></p>
      <ul>
        <li>
          <strong>Background:</strong> Second-year undergrad majoring in
          Software Engineering. Frequently studies with peers in group rooms,
          replaying lecture captures on a shared screen.
        </li>
        <li>
          <strong>Needs:</strong> Skim through videos rapidly to find important
          segments (exam tips, worked examples) while still staying in sync with
          teammates.
        </li>
        <li>
          <strong>Pain Points:</strong>
          <ul>
            <li>
              Group multitasking: phone notifications, side conversations, and
              irrelevant slide transitions make it hard to follow the key
              moment.
            </li>
            <li>
              The uniform brightness and motion of the full screen compete for
              his attention.
            </li>
          </ul>
        </li>
        <li>
          <strong>Behavioral Traits:</strong>
          <ul>
            <li>
              Constantly shifts gaze between screen, phone, and
              classmates—roughly one change every 3–5 seconds.
            </li>
            <li>Often rewinds after losing track of what the lecturer said.</li>
            <li>
              Reacts strongly to any interface that visually highlights what
              others are focusing on.
            </li>
          </ul>
        </li>
        <li>
          <strong>Quote:</strong> “If the video could automatically zoom in on
          what matters, we’d all stop asking ‘wait, where is he pointing?’”
        </li>
      </ul>

      <h3>1.4 Use Case Scenario</h3>
      <p><strong>Context:</strong></p>
      <p>
        Guo sits in the Schulich Library study area with his laptop and
        notebook. He’s replaying a 50-minute linear-systems lecture through the
        prototype immersive player that includes gaze-aware delivery.
      </p>
      <p><strong>Step-by-Step Interaction:</strong></p>
      <ul>
        <li>
          <strong>A1 (User Action): </strong> Guo scrubs to the 25-minute mark
          where a complex derivation begins, then leans forward and fixates on a
          two-line differential equation near the center of the slide.
          <ul>
            <li>
              <strong>R1 (System Response):</strong> The gaze-tracking module
              detects sustained focus (≥ 600 ms dwell) and defines this as the
              Region of Interest (ROI).
              <ul>
                <li>
                  Streaming controller reallocates bandwidth to that ROI:
                  resolution and contrast increase by 20%, ensuring symbols stay
                  sharp.
                </li>
                <li>
                  Peripheral regions receive 40% lower bitrate and a slight
                  soft-focus blur, reducing visual noise.
                </li>
                <li>
                  A calm fade transition (200 ms) prevents abrupt flicker.
                </li>
              </ul>
            </li>
          </ul>
        </li>
        <li>
          <strong>A2 (User Action):</strong> He replays the previous five
          seconds to confirm a coefficient.
          <ul>
            <li>
              <strong>R2 (System Response):</strong> Prefetches the upcoming GOP
              for the same ROI so there’s no buffering. The ROI “stickiness”
              timer (1–2 s) maintains focus, so the highlighted area doesn’t
              jump during the rewind.
            </li>
          </ul>
        </li>
        <li>
          <strong>A3 (User Action):</strong> He writes notes in his notebook,
          briefly glances away, then returns attention to a figure on the right
          side of the slide.
          <ul>
            <li>
              <strong>R3 (System Response):</strong> The system smoothly
              transitions the ROI from equation to figure using a gaze-vector
              interpolation (300 ms ramp).
              <ul>
                <li>
                  The figure area sharpens while the previous ROI gently fades
                  back to normal clarity.
                </li>
                <li>
                  Captions and axes labels remain fully legible to preserve
                  context.
                </li>
              </ul>
            </li>
          </ul>
        </li>
        <li>
          <strong>A4 (User Action):</strong> Guo increases playback speed to
          1.5× to review faster.
          <ul>
            <li>
              <strong>R4 (System Response):</strong> Adaptive pre-fetching
              widens the ROI slightly to anticipate quicker eye movements and
              keeps latency below 100 ms.
            </li>
          </ul>
        </li>
      </ul>
      <p><strong>Outcome:</strong></p>
      <ul>
        <li>
          Guo experiences uninterrupted focus and reports less eye strain and
          fewer rewinds compared with the standard player.
        </li>
        <li>Bandwidth metrics show ~30% reduction in non-ROI data transfer.</li>
        <li>
          Subjectively, Guo describes the player as “quietly helpful—like it
          reads my mind but doesn’t get in the way.”
        </li>
      </ul>
      <div class="imagebox">
        <img
          src="images/Watching recorded lecture.png"
          height="250"
          border="0"
          alt="picture of me"
        />

        <div>figure3: Watching recorded lecture</div>
      </div>
      <h3>1.5 Related Products</h3>
      <ul>
        <li>
          <strong>YouTube “Smart Player”:</strong> Allows users to skip less
          relevant parts, but not gaze-aware or adaptive.
        </li>
        <li>
          <strong>Foveated Rendering in VR:</strong> Used in XR headsets to
          improve graphics performance, but not applied to streaming
          lectures/videos.
        </li>
        <li>
          <strong>Attention tracking tools (e.g., Proctorio):</strong> Track
          gaze for exam monitoring, but not for improving learning experience.
        </li>
      </ul>

      <p>
        <strong>Limitations:</strong> Existing solutions either focus on
        rendering performance or surveillance,
        <strong>not calm, user-centered adaptation.</strong>
      </p>

      <h3>1.6 Comparison with Proposed Solution</h3>
      <ul>
        <li>
          User-centered: Improves focus rather than monitoring or punishing.
        </li>
        <li>
          Calm technology: Works in the background without intrusive alerts.
        </li>
        <li>
          Bandwidth-efficient: Allocates resources to ROI instead of wasting
          them on ignored regions.
        </li>
      </ul>
      <div class="imagebox">
        <img
          src="images/Foveated Rendering in VR.png"
          height="200"
          border="0"
          alt="picture of me"
        />
        <div>figure4: Foveated Rendering in VR</div>
      </div>

      <h3>1.7 High-Level Design</h3>
      <ul>
        <li>
          <strong>Input Layer:</strong> Eye-tracking / viewport data (from
          webcam or XR headset).
        </li>
        <li>
          <strong>Processing Layer:</strong> ROI detection → adaptive streaming
          controller.
        </li>
        <li>
          <strong>Delivery Layer:</strong> Dynamic video player (high-resolution
          ROI, low-resolution periphery).
        </li>
        <li>
          <strong>Feedback Loop:</strong> User gaze continuously updates ROI
          focus.
        </li>
      </ul>
      <div class="imagebox">
        <img
          src="images/3D gaze path visualization.png"
          width="300"
          height="300"
          border="0"
          alt="picture of me"
        />
        <div>figure5: 3D gaze path visualization</div>
      </div>
      <h3>1.8 Feasibility</h3>
      <ul>
        <li>
          <strong>Skills:</strong>
          <ul>
            <li>
              Team has experience with video streaming (research background in
              immersive video).
            </li>
            <li>
              Basic gaze estimation available via open-source APIs or headset
              SDKs.
            </li>
          </ul>
        </li>
        <li>
          <strong>Effort Estimate:</strong> 20–25 hours per member for prototype
          (integration + testing).
        </li>
        <li>
          <strong>Scope:</strong> Focus on design prototype + small pilot study,
          not full system deployment.
        </li>
        <li><strong>Conclusion:</strong> Feasible within semester scope.</li>
      </ul>

      <h2>2. HCI Ethics Table</h2>

      <table>
        <tr>
          <th>Group / Student Names</th>
          <td>Group K: Hengyuan Guo, Jiawei He, and Kaiyuan Hu</td>
        </tr>
        <tr>
          <th>Title of Project</th>
          <td>Gaze-Aware Immersive Video Delivery (Calm Technology)</td>
        </tr>
        <tr>
          <th>Project / Study Description</th>
          <td>
            The project addresses the problem of video systems streaming
            uniformly, regardless of where a user is looking. The proposed
            solution is a system that leverages gaze and viewport data to
            deliver higher fidelity video where the user is looking (their
            region of interest), while calmly reducing detail in the periphery
            and operating as a calm technology in the background.
          </td>
        </tr>
        <tr>
          <th>Participants</th>
          <td>
            Small pilot group including graduate students, research teammates,
            and undergraduate peers aged 19 to 26, across all genders.
          </td>
        </tr>
        <tr>
          <th>Recruitment</th>
          <td>
            Participants recruited from the McGill University community via
            convenience sampling (classmates and lab members who volunteer).
            Sessions conducted in controlled campus environments (labs or study
            rooms).
          </td>
        </tr>
        <tr>
          <th>Potential Risks</th>
          <td>
            <ul>
              <li>
                <strong>Data Security Risk:</strong> Gaze data could be
                vulnerable to unauthorized access if stored.
              </li>
              <li>
                <strong>Minor Discomfort:</strong> Possible eye strain or
                physical discomfort from eye-tracking equipment.
              </li>
              <li>
                <strong>Psychological Risk:</strong> Awareness of being tracked
                might alter behavior and cause mild anxiety.
              </li>
            </ul>
          </td>
        </tr>
        <tr>
          <th>Risk Mitigation</th>
          <td>
            <ul>
              <li>
                <strong>Informed Consent:</strong> Clearly explain what data is
                collected, why, how it’s used, and who has access before
                participation.
              </li>
              <li>
                <strong>User Control and Transparency:</strong> Provide a simple
                option to disable gaze-aware features; emphasize improvement,
                not monitoring.
              </li>
              <li>
                <strong>Limiting Session Duration:</strong> Keep pilot sessions
                short and allow breaks to minimize strain.
              </li>
            </ul>
          </td>
        </tr>
      </table>

      <footer>
        <p>© Fall 2025 Group K - McGill University</p>
        <p>ECSE 542 Human Computer Interaction</p>
      </footer>
    </div>
  </body>
</html>
