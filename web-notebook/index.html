<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Group K - HCI Project Web Notebook</title>
    <style>
      body {
        font-family: "Helvetica Neue", Arial, sans-serif;
        line-height: 1.6;
        background-color: #f8f9fa;
        color: #333;
        margin: 0;
        padding: 0;
      }
      .container {
        max-width: 900px;
        margin: 40px auto;
        background: #fff;
        padding: 40px;
        box-shadow: 0 0 8px rgba(0, 0, 0, 0.1);
        border-radius: 8px;
      }
      .imagebox {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
      }
      h1,
      h2,
      h3,
      h4 {
        color: #222;
        font-weight: 600;
        margin-top: 1.5em;
      }
      h1 {
        text-align: center;
        margin-bottom: 0.5em;
      }
      h2 {
        border-bottom: 2px solid #e5e5e5;
        padding-bottom: 4px;
        margin-bottom: 0.6em;
      }
      p,
      ul,
      ol {
        margin-bottom: 1em;
      }
      ul {
        padding-left: 1.2em;
      }
      table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 1em;
      }
      th,
      td {
        border: 1px solid #ddd;
        padding: 8px 10px;
        vertical-align: top;
      }
      th {
        background-color: #f2f2f2;
        text-align: left;
      }
      footer {
        text-align: center;
        font-size: 0.9em;
        color: #777;
        margin-top: 3em;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>Gaze-Aware Immersive Video Delivery</h1>
      <p style="text-align: center; font-size: 1.1em; color: #555">
        HCI Project Web Notebook
      </p>
      <p style="text-align: center; font-size: 0.9em; color: #777">
        Group K — Hengyuan Guo, Jiawei He, Kaiyuan Hu
      </p>

      <h2>1. User Observation Report – ECSE 542</h2>
      <p>
        <strong>User Group:</strong> Students at McGill (classmates and lab
        members)
      </p>

      <h3>1.1 Observations of Users</h3>
      <p>I observed three groups of users:</p>
      <ul>
        <li>
          <strong>Graduate students in Schulich Library</strong> watching
          recorded lectures. They frequently paused, rewound, and focused their
          gaze on slides or equations.
        </li>
        <li>
          <strong
            >Research teammates in my office (McConnell Engineering
            Building)</strong
          >
          watching websites while coding. They alternated gaze between the
          video, IDE, and notes, often missing peripheral details.
        </li>
        <li>
          <strong
            >Undergraduate peers in group study spaces (McGill Campus &
            apartment)</strong
          >
          reviewing lecture capture. They multitasked with phones, laptops, and
          side conversations, frequently breaking immersion.
        </li>
      </ul>

      <p>
        <strong>Central observation:</strong> In all cases, users focused only
        on a small region of the video, while peripheral content was ignored.
        This mismatch between system delivery (uniform streaming) and actual
        attention patterns creates wasted bandwidth and cognitive overload.
      </p>

      <h3>1.2 Identifying the Problem</h3>
      <p>
        <strong>Problem:</strong> Current video systems stream uniformly,
        regardless of user attention. Users experience visual clutter and
        network inefficiency, while their region of interest (ROI) receives no
        prioritization.
      </p>
      <ul>
        <li>
          <strong>Example:</strong> A PhD student stared at one diagram for ~30
          seconds, while the rest of the video streamed at high resolution but
          was ignored.
        </li>
        <li>
          <strong>Opportunity:</strong> Leverage gaze/viewport data to deliver
          higher fidelity where users are looking, while calmly reducing detail
          elsewhere.
        </li>
      </ul>

      <h3>1.3 Personas</h3>
      <p><strong>Persona 1 – Guo (Graduate Student, 24)</strong></p>
      <ul>
        <li>Needs: Understand equations in recorded lectures.</li>
        <li>Pain: Cluttered visuals and buffering reduce concentration.</li>
        <li>Behavior: Fixates gaze on slide equations for long periods.</li>
      </ul>

      <p><strong>Persona 2 – Richard (Ph.D. Student, 26)</strong></p>
      <ul>
        <li>Needs: Watch research demo videos while coding.</li>
        <li>
          Pain: Hard to focus when multiple windows compete for attention.
        </li>
        <li>Behavior: Alternates gaze between main video and notes.</li>
      </ul>

      <p><strong>Persona 3 – David (Undergraduate, 19)</strong></p>
      <ul>
        <li>Needs: Review lecture capture quickly with group.</li>
        <li>Pain: Distracted by multitasking and irrelevant video sections.</li>
        <li>
          Behavior: Constantly shifts gaze between video, phone, and peers.
        </li>
      </ul>

      <h3>1.4 Use Case Scenario</h3>
      <p><strong>Scenario – Guo watches a recorded lecture:</strong></p>
      <ul>
        <li>
          Without system: Entire video streams uniformly, bandwidth wasted, Guo
          struggles to maintain focus.
        </li>
        <li>
          With proposed system:
          <ul>
            <li>Eye-tracking detects gaze on equations.</li>
            <li>ROI is enhanced in resolution and contrast.</li>
            <li>Peripheral video is de-emphasized but still available.</li>
            <li>
              Result: Guo stays focused, consumes less bandwidth, and completes
              review more efficiently.
            </li>
          </ul>
        </li>
      </ul>

      <h3>1.5 Related Products</h3>
      <ul>
        <li>
          <strong>YouTube “Smart Player”:</strong> Allows users to skip less
          relevant parts, but not gaze-aware or adaptive.
        </li>
        <li>
          <strong>Foveated Rendering in VR:</strong> Used in XR headsets to
          improve graphics performance, but not applied to streaming
          lectures/videos.
        </li>
        <li>
          <strong>Attention tracking tools (e.g., Proctorio):</strong> Track
          gaze for exam monitoring, but not for improving learning experience.
        </li>
      </ul>

      <p>
        <strong>Limitations:</strong> Existing solutions either focus on
        rendering performance or surveillance, not calm, user-centered
        adaptation.
      </p>

      <h3>1.6 Comparison with Proposed Solution</h3>
      <ul>
        <li>
          User-centered: Improves focus rather than monitoring or punishing.
        </li>
        <li>
          Calm technology: Works in the background without intrusive alerts.
        </li>
        <li>
          Bandwidth-efficient: Allocates resources to ROI instead of wasting
          them on ignored regions.
        </li>
      </ul>
      <div class="imagebox">
        <img
          src="images/Foveated Rendering in VR.png"
          height="200"
          border="0"
          alt="picture of me"
        />
        <div>figure1: Foveated Rendering in VR</div>
      </div>

      <h3>1.7 High-Level Design</h3>
      <ul>
        <li>
          <strong>Input Layer:</strong> Eye-tracking / viewport data (from
          webcam or XR headset).
        </li>
        <li>
          <strong>Processing Layer:</strong> ROI detection → adaptive streaming
          controller.
        </li>
        <li>
          <strong>Delivery Layer:</strong> Dynamic video player (high-resolution
          ROI, low-resolution periphery).
        </li>
        <li>
          <strong>Feedback Loop:</strong> User gaze continuously updates ROI
          focus.
        </li>
      </ul>
      <div class="imagebox">
        <img
          src="images/3D gaze path visualization.png"
          width="300"
          height="300"
          border="0"
          alt="picture of me"
        />
        <div>figure2: 3D gaze path visualization</div>
      </div>
      <h3>1.8 Feasibility</h3>
      <ul>
        <li>
          <strong>Skills:</strong>
          <ul>
            <li>
              Team has experience with video streaming (research background in
              immersive video).
            </li>
            <li>
              Basic gaze estimation available via open-source APIs or headset
              SDKs.
            </li>
          </ul>
        </li>
        <li>
          <strong>Effort Estimate:</strong> 20–25 hours per member for prototype
          (integration + testing).
        </li>
        <li>
          <strong>Scope:</strong> Focus on design prototype + small pilot study,
          not full system deployment.
        </li>
        <li><strong>Conclusion:</strong> Feasible within semester scope.</li>
      </ul>

      <h2>2. HCI Ethics Table</h2>

      <table>
        <tr>
          <th>Group / Student Names</th>
          <td>Group K: Hengyuan Guo, Jiawei He, and Kaiyuan Hu</td>
        </tr>
        <tr>
          <th>Title of Project</th>
          <td>Gaze-Aware Immersive Video Delivery (Calm Technology)</td>
        </tr>
        <tr>
          <th>Project / Study Description</th>
          <td>
            The project addresses the problem of video systems streaming
            uniformly, regardless of where a user is looking. The proposed
            solution is a system that leverages gaze and viewport data to
            deliver higher fidelity video where the user is looking (their
            region of interest), while calmly reducing detail in the periphery
            and operating as a calm technology in the background.
          </td>
        </tr>
        <tr>
          <th>Participants</th>
          <td>
            Small pilot group including graduate students, research teammates,
            and undergraduate peers aged 19 to 26, across all genders.
          </td>
        </tr>
        <tr>
          <th>Recruitment</th>
          <td>
            Participants recruited from the McGill University community via
            convenience sampling (classmates and lab members who volunteer).
            Sessions conducted in controlled campus environments (labs or study
            rooms).
          </td>
        </tr>
        <tr>
          <th>Potential Risks</th>
          <td>
            <ul>
              <li>
                <strong>Data Security Risk:</strong> Gaze data could be
                vulnerable to unauthorized access if stored.
              </li>
              <li>
                <strong>Minor Discomfort:</strong> Possible eye strain or
                physical discomfort from eye-tracking equipment.
              </li>
              <li>
                <strong>Psychological Risk:</strong> Awareness of being tracked
                might alter behavior and cause mild anxiety.
              </li>
            </ul>
          </td>
        </tr>
        <tr>
          <th>Risk Mitigation</th>
          <td>
            <ul>
              <li>
                <strong>Informed Consent:</strong> Clearly explain what data is
                collected, why, how it’s used, and who has access before
                participation.
              </li>
              <li>
                <strong>User Control and Transparency:</strong> Provide a simple
                option to disable gaze-aware features; emphasize improvement,
                not monitoring.
              </li>
              <li>
                <strong>Limiting Session Duration:</strong> Keep pilot sessions
                short and allow breaks to minimize strain.
              </li>
            </ul>
          </td>
        </tr>
      </table>

      <footer>
        <p>© Fall 2025 Group K - McGill University</p>
        <p>ECSE 542 Human Computer Interaction</p>
      </footer>
    </div>
  </body>
</html>
